# Plan: Performance Regression Suite

## 1. Introduction
As features are added, performance (startup time, extraction speed, analysis speed) can degrade. We need to track this.

## 2. Objectives
- Integrate `pytest-benchmark`.
- Create a suite of benchmarks for critical paths.
- Fail CI if performance degrades significantly (optional, or just report).

## 3. Implementation Details

### 3.1. Dependencies
- Add `pytest-benchmark` to `requirements-dev.txt`.

### 3.2. Benchmark Tests
Create `tests/performance/test_benchmarks.py`:
- **Startup**: Measure time to `import fetchext`.
- **Parsing**: Measure `CrxDecoder.parse` on a large CRX.
- **Hashing**: Measure `sha256_file` on a large file.
- **Analysis**: Measure `ComplexityAnalyzer` on a complex JS file.

### 3.3. CI Integration
- Update `Makefile` and CI workflow to run benchmarks.
- Ideally, compare against a baseline (though this is hard in stateless CI, we can at least print the stats).

## 4. Verification
- Run `pytest tests/performance`.
- Ensure output shows mean/stddev timings.
