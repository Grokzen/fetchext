# Plan: Domain Extractor (v1.2.0)

## Goal

Implement a domain and URL extractor to analyze extension source code for network indicators. This helps in forensic analysis to identify where an extension is sending data or fetching resources from.

## Strategy

1. **Regex Matching**: Use robust regular expressions to find URLs (http/https/ws/wss) and potential domains in text files.
2. **File Traversal**: Iterate through all text-based files (JS, HTML, CSS, JSON) in the extension archive.
3. **Filtering**: Filter out common noise (e.g., `w3.org` schemas, `google.com` in comments if possible, though simple regex might catch them).
4. **Reporting**: Output a list of unique domains and full URLs found, grouped by file or aggregated.

## Implementation Details

### 1. Core Logic (`src/fetchext/analysis/domains.py`)

- `extract_domains(file_path: Path) -> Dict[str, Set[str]]`:
  - Opens ZIP/CRX.
  - Iterates through files.
  - If file extension is `.js`, `.html`, `.css`, `.json`, `.xml`:
    - Read content.
    - Apply regex to find URLs.
    - Extract domain from URL.
  - Returns dictionary: `{'domains': set(...), 'urls': set(...)}`.

### 2. CLI Integration (`src/fetchext/cli.py`)

- Update `analyze` command to accept `--domains` flag.
- Output results in a table or list.
  - Section 1: Unique Domains (sorted).
  - Section 2: Full URLs (optional or verbose).

### 3. Testing

- Unit tests with dummy text containing various URL formats.
- Integration test with a dummy extension.

## Verification

- Run `fext analyze --domains <file>` on a known extension (e.g., uBlock Origin).
- Verify it lists `github.com`, `raymondhill.net`, etc.
