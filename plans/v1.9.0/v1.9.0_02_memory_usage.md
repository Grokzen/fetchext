# Plan: Memory Usage Optimization

## Objective

Reduce memory footprint during extraction and analysis of large archives.

## Problem

Currently, some operations might be loading entire files into memory, which can be problematic for large extensions (e.g., > 100MB).

## Analysis

1. **CRX Parsing**: `CrxDecoder` already uses `PartialFileReader` for streaming, which is good.
2. **Extraction**: `zipfile.ZipFile.extractall` is generally efficient, but we should check if we are doing any manual reading.
3. **Analysis**:
    * `entropy`: Reads the whole file content?
    * `complexity`: Reads the whole file content?
    * `yara`: Reads the whole file content?
    * `secrets`: Reads the whole file content?

## Implementation Steps

1. **Audit Analysis Modules**: Check `entropy.py`, `complexity.py`, `yara.py`, `secrets.py` for full file reads.
2. **Implement Streaming**:
    * For `entropy`: Calculate entropy in chunks.
    * For `yara`: Use `match_data` or file path matching instead of reading content if possible.
    * For `secrets`: Scan line by line or in chunks.
3. **Profile**: Use `memory_profiler` or `tracemalloc` to verify improvements.

## Verification

* Create a large dummy extension (e.g., with a 500MB file).
* Run `fext analyze` commands and monitor memory usage.
